{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:50:12|TQ-WARNING| Error enabling CUDA. cuda.is_available() returned False. CPU will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘tract_merged.csv’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Griffin Davis, The University of Texas at Dallas\n",
    "# (C) 2022\n",
    "# Data source:\n",
    "# Chetty, Raj; Friedman, John; Hendren, Nathaniel; Jones, Maggie R.; Porter, Sonya R., 2022, \n",
    "# \"Replication Data for: The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility\", \n",
    "# https://doi.org/10.7910/DVN/NKCQM1, Harvard Dataverse, V1, UNF:6:wwWmCZy1LUqtq02qHdCKFQ== [fileUNF] \n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "nnF = nn.functional\n",
    "\n",
    "from torchquad import set_up_backend\n",
    "from torchquad import Simpson, MonteCarlo\n",
    "from torchquad.utils.set_precision import set_precision\n",
    "import torchquad\n",
    "\n",
    "set_up_backend(\"torch\", data_type=\"float32\")\n",
    "\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "reload(logging) # Notebook workaround\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(threadName)s] [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(f'logs/{datetime.now().strftime(\"%Y-%m-%d\")}_em_mdn.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Download data\n",
    "!wget -nc https://personal.utdallas.edu/~gcd/data/tract_merged.csv\n",
    "ds = pd.read_csv('tract_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subset of columns\n",
    "cols = ['id', 'hhinc_mean2000', 'mean_commutetime2000', 'frac_coll_plus2000', 'frac_coll_plus2010', \n",
    "        'med_hhinc1990', 'med_hhinc2016', 'popdensity2000', 'poor_share2010', 'poor_share2000', \n",
    "        'poor_share1990', 'gsmn_math_g3_2013', 'traveltime15_2010', 'emp2000', 'singleparent_share1990',\n",
    "        'singleparent_share2010', 'singleparent_share2000', \n",
    "        'mail_return_rate2010', 'jobs_total_5mi_2015', 'jobs_highpay_5mi_2015', \n",
    "        'popdensity2010', 'job_density_2013', 'kfr_pooled_pooled_p1', \n",
    "        'kfr_pooled_pooled_p25', 'kfr_pooled_pooled_p50', 'kfr_pooled_pooled_p75', 'kfr_pooled_pooled_p100']\n",
    "\n",
    "excluded = ['rent_twobed2015', 'ln_wage_growth_hs_grad', 'ann_avg_job_growth_2004_2013']\n",
    "\n",
    "full_cols = cols + excluded\n",
    "\n",
    "# Handle null data\n",
    "ds_full = ds[ds.columns[ds.columns.isin(full_cols)]]\n",
    "ds = ds[ds.columns[ds.columns.isin(cols)]]\n",
    "ds = ds.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for full dataset (including missing data)\n",
    "\n",
    "# Shuffle split the data into training and test sets (75% / 25%)\n",
    "train, test = train_test_split(ds_full)\n",
    "\n",
    "train_X = train.loc[:,'hhinc_mean2000':'job_density_2013']\n",
    "test_X = test.loc[:,'hhinc_mean2000':'job_density_2013']\n",
    "\n",
    "percentiles = ['kfr_pooled_pooled_p1', 'kfr_pooled_pooled_p25', 'kfr_pooled_pooled_p50', 'kfr_pooled_pooled_p75']\n",
    "train_Y = train.loc[:, percentiles[0]]\n",
    "test_Y = test.loc[:, percentiles[0]]\n",
    "\n",
    "# Reset indexes and convert Y to pd.Series\n",
    "train_X.reset_index(drop=True, inplace=True)\n",
    "train_Y = train_Y.reset_index(drop=True).squeeze()\n",
    "test_X.reset_index(drop=True, inplace=True)\n",
    "test_Y = test_Y.reset_index(drop=True).squeeze()\n",
    "\n",
    "features = train_X.shape[1]\n",
    "data_points = train_X.shape[0]\n",
    "\n",
    "# Get missing indexes\n",
    "train_Xmissing = np.isnan(train_X)\n",
    "train_imissing = []\n",
    "for m in range(data_points):\n",
    "    row = []\n",
    "    for i in range(features):\n",
    "        if train_Xmissing.iloc[m][i]:\n",
    "            row.append(i)\n",
    "    train_imissing.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture Density Network with expectation maximization algorithm to handle missing data\n",
    "\n",
    "# Store feature distribution parameters\n",
    "feat_params = []\n",
    "\n",
    "LOSS_SAMPLE_SIZE = 50\n",
    "loss_generator = np.random.default_rng()\n",
    "integrator = MonteCarlo()\n",
    "\n",
    "def init_feat_params(X):\n",
    "    # Initialize feature parameters\n",
    "    for n in range(features):\n",
    "        mu = np.nanmean(X.iloc[:, n])\n",
    "        sigma = np.nanstd(X.iloc[:, n])\n",
    "        feat_params.append((mu, sigma))\n",
    "\n",
    "def p_x(i, xi):\n",
    "    mu, sigma = feat_params[i]\n",
    "    dist = Normal(mu, sigma)\n",
    "    p_xi  = torch.exp(dist.log_prob(torch.tensor(np.double(xi))))\n",
    "    return p_xi\n",
    "\n",
    "class EMNet(nn.Module):\n",
    "    def __init__(self, features, hidden_dim, out_dim):\n",
    "        super(EMNet, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(features, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.have_stored_q = False\n",
    "        self.stored_q = torch.empty((1, 1), dtype=torch.float32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        params = self.seq(x)\n",
    "        mu, sigma = torch.tensor_split(params, params.shape[0], dim=0)\n",
    "        \n",
    "        return mu, sigma+1\n",
    "\n",
    "    # Evaluate q density for EM algorithm\n",
    "    # For a given data point (x, y)\n",
    "    def q(self, x, y, xmis, imis, mu, sigma):\n",
    "        # p(y | x) with current mu, sigma\n",
    "        y_dist = Normal(mu, sigma)\n",
    "\n",
    "        # =======================\n",
    "        # Numerator of q function\n",
    "        # =======================\n",
    "\n",
    "        # p(y | x)\n",
    "        num = torch.exp(y_dist.log_prob(torch.tensor(np.double(y))))\n",
    "\n",
    "        # Product[ p(x_mis) ]\n",
    "        st = time.time()\n",
    "        for k in range(len(xmis)):\n",
    "            num *= p_x(imis[k], xmis[k])\n",
    "\n",
    "        # =========================\n",
    "        # Denominator of q function\n",
    "        # =========================\n",
    "\n",
    "        # Function for the integral\n",
    "        def int_func(*xmis_hat):\n",
    "            # Get product of all p(xmis)\n",
    "            prod = torch.ones(1, 1)\n",
    "            for i in range(len(xmis_hat)):\n",
    "                prod *= p_x(imis[i], xmis_hat[i])\n",
    "            x_hat = x\n",
    "            f_i = 0\n",
    "            # Replace missing values with current guesses\n",
    "            for f in range(len(x_hat)):\n",
    "                if np.isnan(x_hat[f]):\n",
    "                    x_hat[f] = xmis_hat[f_i]\n",
    "                    f_i+=1\n",
    "            # Run X with current guesses through NN\n",
    "            mu_hat, sigma_hat = self.forward(torch.tensor(np.double(x_hat)))\n",
    "            dist = Normal(mu_hat, sigma_hat)\n",
    "            # Return p(y|x)\n",
    "            return torch.exp(dist.log_prob(torch.tensor(np.double(y))))\n",
    "\n",
    "        # Don't calculate the integral every time\n",
    "        if self.have_stored_q:\n",
    "            return num / self.stored_q\n",
    "\n",
    "        # Setup list of ranges corresponding to num of missing values\n",
    "        ranges = [[-np.inf, np.inf]] * len(imis)\n",
    "\n",
    "        # Integrate over each xmis, -inf to inf\n",
    "        # st = time.time()\n",
    "        sampling_points = 3**10\n",
    "        dimension = len(imis)\n",
    "\n",
    "        den = integrator.integrate(int_func, dim=dimension, \n",
    "                    N=sampling_points, integration_domain=ranges)\n",
    "        \n",
    "        self.stored_q = den\n",
    "        self.have_stored_q = True\n",
    "        \n",
    "        # logging.info(f\"integral: {round(time.time()-st,1)}s, err={err}\")\n",
    "\n",
    "        return num / den\n",
    "\n",
    "    # Produce gradients of feature distribution parameters\n",
    "    def grad_mu_xi(self, X, Y, mu_xi, sigma_xi, xi, imis):\n",
    "        m = 0\n",
    "        def int_func(*xmis_hat):\n",
    "            nn_mu, nn_sigma = self.forward(torch.tensor(np.double(X[m])))\n",
    "            \n",
    "            qm = self.q(X[m], Y[m], xmis_hat, imis[m], nn_mu, nn_sigma)\n",
    "            return qm * ((xi[m] / (sigma_xi**2)) - (mu_xi / (sigma_xi**2)))\n",
    "\n",
    "        sum_ints = torch.zeros(1, 1)\n",
    "\n",
    "        for m in range(len(X)):\n",
    "            # Setup list of ranges corresponding to num of missing values\n",
    "            ranges = [[-np.inf, np.inf]] * len(imis)\n",
    "\n",
    "            sampling_points = 3**10\n",
    "            dimension = len(imis)\n",
    "\n",
    "            res = integrator.integrate(int_func, dim=dimension, \n",
    "                        N=sampling_points, integration_domain=ranges)\n",
    "            sum_ints += res\n",
    "\n",
    "        return sum_ints\n",
    "\n",
    "    def grad_sigma_xi(self, X, Y, mu_xi, sigma_xi, xi, imis):\n",
    "        m = 0\n",
    "        def int_func(*xmis_hat):\n",
    "            nn_mu, nn_sigma = self.forward(torch.tensor(np.double(X[m])))\n",
    "\n",
    "            qm = self.q(X[m], Y[m], xmis_hat, imis[m], nn_mu, nn_sigma)\n",
    "            return qm * ((-1 / sigma_xi) - ((xi[m] - mu_xi) / (sigma_xi**3)))\n",
    "\n",
    "        sum_ints = torch.zeros(1, 1)\n",
    "\n",
    "        for m in range(len(X)):\n",
    "            # Setup list of ranges corresponding to num of missing values\n",
    "            ranges = [[-np.inf, np.inf]] * len(imis)\n",
    "\n",
    "            sampling_points = 3**10\n",
    "            dimension = len(imis)\n",
    "\n",
    "            res = integrator.integrate(int_func, dim=dimension, \n",
    "                        N=sampling_points, integration_domain=ranges)\n",
    "            sum_ints += res\n",
    "\n",
    "        return sum_ints\n",
    "    \n",
    "    def loss(self, X, Y, imis):\n",
    "        logging.info(\"Calculating loss...\")\n",
    "        m = 0\n",
    "        def int_func(*xmis_hat):\n",
    "            # Replace NaNs with xmis_hat\n",
    "            f_i = 0\n",
    "            # Replace missing values with current guesses\n",
    "            for f in range(len(X[m])):\n",
    "                if np.isnan(X[m, f]):\n",
    "                    X[m, f] = xmis_hat[f_i]\n",
    "                    f_i+=1\n",
    "            # Run X with current guesses through NN\n",
    "            mu, sigma = self.forward(torch.tensor(np.double(X[m])))\n",
    "            dist = Normal(mu, sigma)\n",
    "            # Get p(y|x)\n",
    "            log_p_y_x = dist.log_prob(torch.tensor(np.double(Y[m])))\n",
    "\n",
    "            # Get q_m(xmis_hat)\n",
    "            qm = self.q(X[m], Y[m], xmis_hat, imis[m], mu, sigma)\n",
    "            return qm * log_p_y_x\n",
    "\n",
    "        sum_ints = torch.zeros(1, 1)\n",
    "\n",
    "        # Randomly sample rows from the training data for each iteration\n",
    "        X_sample = loss_generator.integers(0, high=X.shape[0], size=LOSS_SAMPLE_SIZE)\n",
    "\n",
    "        for m in X_sample:\n",
    "            # Handle case of no missing values\n",
    "            if len(imis[m]) == 0:\n",
    "                logging.info(f\"row {m} : no missing values\")\n",
    "                mu, sigma = self.forward(torch.tensor(np.double(X[m])))\n",
    "                dist = Normal(mu, sigma)\n",
    "                # Get p(y|x)\n",
    "                log_p_y_x = dist.log_prob(torch.tensor(np.double(Y[m])))\n",
    "\n",
    "                sum_ints += log_p_y_x\n",
    "            else:\n",
    "                # Setup list of ranges corresponding to num of missing values\n",
    "                ranges = [[-np.inf, np.inf]] * len(imis)\n",
    "                start_time = time.time()\n",
    "                self.have_stored_q = False\n",
    "                sampling_points = 3**10\n",
    "                dimension = len(imis)\n",
    "\n",
    "                res = integrator.integrate(int_func, dim=dimension, \n",
    "                            N=sampling_points, integration_domain=ranges)\n",
    "                logging.info(f\"row {m} : completed {len(imis[m])} dim integral in {int(round(time.time()-start_time, 0))}s, err={err}\")\n",
    "                sum_ints += res\n",
    "\n",
    "        return -1 * sum_ints\n",
    "\n",
    "    def predict(self, x, imis):\n",
    "        # def int_func(xmis_hat):\n",
    "        #     # Replace NaNs with xmis_hat\n",
    "        #     f_i = 0\n",
    "        #     # Replace missing values with current guesses\n",
    "        #     for f in range(len(x)):\n",
    "        #         if np.isnan(x[f]):\n",
    "        #             x[f] = xmis_hat[f_i]\n",
    "        #             f_i+=1\n",
    "        #     # Run X with current guesses through NN\n",
    "        #     mu, sigma = self.forward(torch.tensor(np.double(x)))\n",
    "        #     dist = Normal(mu, sigma)\n",
    "\n",
    "        #     return qm\n",
    "\n",
    "        # # Setup list of ranges corresponding to num of missing values\n",
    "        # ranges = [(-np.inf, np.inf) for i in range(len(imis))]\n",
    "        # integrate.nquad(int_func, ranges)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training constants\n",
    "TRAIN_ITERATIONS = 100\n",
    "FEAT_PARAM_STEP = 0.001\n",
    "\n",
    "# Train neural network\n",
    "def train_mdn(mdn, X, Y, optimizer, verbose):\n",
    "    mdn.train()\n",
    "\n",
    "    init_feat_params(X)\n",
    "\n",
    "    X = X.to_numpy()\n",
    "    Y = Y.to_numpy()\n",
    "    \n",
    "    for index in range(TRAIN_ITERATIONS):\n",
    "        start_time = time.time()\n",
    "        # Calculate NN loss        \n",
    "        loss = mdn.loss(X, Y, train_imissing)\n",
    "        \n",
    "        # Backpropagate\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update feature parameters using gradient descent\n",
    "        for f in range(features):\n",
    "            cur_mu, cur_sigma = feat_params[f]\n",
    "            xi = X[:,f]\n",
    "            grad_mu = mdn.grad_mu_xi(X, Y, cur_mu, cur_sigma, xi, train_imissing)\n",
    "            grad_sigma = mdn.grad_sigma_xi(X, Y, cur_mu, cur_sigma, xi, train_imissing)\n",
    "\n",
    "            feat_params[f] = (cur_mu+grad_mu, cur_sigma+grad_sigma)\n",
    "\n",
    "        if verbose:\n",
    "            logging.info(f\"iteration: {index} nn loss: {loss} time elapsed: {int(round((time.time()-start_time)/60, 0))}min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate neural network\n",
    "def test_mdn(mdn, X, Y, verbose):\n",
    "    mdn.eval()\n",
    "\n",
    "    X = X.to_numpy()\n",
    "    Y = Y.to_numpy()\n",
    "\n",
    "    # Get missing indexes\n",
    "    Xmissing = np.isnan(X)\n",
    "    imissing = []\n",
    "    for m in range(X.shape[0]):\n",
    "        row = []\n",
    "        for i in range(features):\n",
    "            if Xmissing[m, i]:\n",
    "                row.append(i)\n",
    "        imissing.append(row)\n",
    "    \n",
    "    sq_er = []\n",
    "    with torch.no_grad():\n",
    "        # Get total loss\n",
    "        test_loss = mdn.loss(X, Y, imissing)\n",
    "\n",
    "        # Get squared error for predictions\n",
    "        # for index in range(X.shape[0]):\n",
    "        #     row = X[index, :]\n",
    "        #     x = torch.tensor(np.double(row))\n",
    "        #     y = torch.tensor(np.double(Y[index]))\n",
    "\n",
    "        #     mu, sigma = mdn.forward(x)\n",
    "\n",
    "        #     sq_er.append((mu.item() - y)**2)\n",
    "\n",
    "    test_loss /= test_X.shape[0]\n",
    "    \n",
    "    if verbose:\n",
    "        logging.info(f\"Avg test loss: {test_loss}\")\n",
    "        # print(f\"Mean squared error: {np.mean(sq_er)}\")\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-22 19:51:14,585 [MainThread] [INFO] Calculating loss...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "%%capture output\n",
    "# Retrain with full training dataset (selected 10)\n",
    "mdn = EMNet(features, 10, 2).double()\n",
    "optimizer = torch.optim.Adam(mdn.parameters(), lr=0.0001)\n",
    "\n",
    "train_mdn(mdn, train_X, train_Y, optimizer, True)\n",
    "\n",
    "# Save the final model\n",
    "torch.save(mdn, f'em_mdn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mobility-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce1c8699971c3abca39737a4f17ac27a5a89024d987616fa9a36ba7fc3fab9ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
