{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘tract_merged.csv’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Griffin Davis, The University of Texas at Dallas\n",
    "# (C) 2022\n",
    "# Data source:\n",
    "# Chetty, Raj; Friedman, John; Hendren, Nathaniel; Jones, Maggie R.; Porter, Sonya R., 2022, \n",
    "# \"Replication Data for: The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility\", \n",
    "# https://doi.org/10.7910/DVN/NKCQM1, Harvard Dataverse, V1, UNF:6:wwWmCZy1LUqtq02qHdCKFQ== [fileUNF] \n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "nnF = nn.functional\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Download data\n",
    "!wget -nc https://personal.utdallas.edu/~gcd/data/tract_merged.csv\n",
    "ds = pd.read_csv('tract_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subset of columns\n",
    "cols = ['id', 'hhinc_mean2000', 'mean_commutetime2000', 'frac_coll_plus2000', 'frac_coll_plus2010', \n",
    "        'med_hhinc1990', 'med_hhinc2016', 'popdensity2000', 'poor_share2010', 'poor_share2000', \n",
    "        'poor_share1990', 'gsmn_math_g3_2013', 'traveltime15_2010', 'emp2000', 'singleparent_share1990',\n",
    "        'singleparent_share2010', 'singleparent_share2000', \n",
    "        'mail_return_rate2010', 'jobs_total_5mi_2015', 'jobs_highpay_5mi_2015', \n",
    "        'popdensity2010', 'job_density_2013', 'kfr_pooled_pooled_p1', \n",
    "        'kfr_pooled_pooled_p25', 'kfr_pooled_pooled_p50', 'kfr_pooled_pooled_p75', 'kfr_pooled_pooled_p100']\n",
    "\n",
    "exluded = ['rent_twobed2015', 'ln_wage_growth_hs_grad', 'ann_avg_job_growth_2004_2013']\n",
    "\n",
    "ds_full = ds\n",
    "\n",
    "# Handle null data\n",
    "ds = ds[ds.columns[ds.columns.isin(cols)]]\n",
    "ds = ds.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle split the data into training and test sets (75% / 25%)\n",
    "train, test = train_test_split(ds)\n",
    "\n",
    "train_X = train.loc[:,'hhinc_mean2000':'job_density_2013']\n",
    "test_X = test.loc[:,'hhinc_mean2000':'job_density_2013']\n",
    "\n",
    "percentiles = ['kfr_pooled_pooled_p25', 'kfr_pooled_pooled_p50', 'kfr_pooled_pooled_p75', 'kfr_pooled_pooled_p100']\n",
    "train_Y = train.loc[:, percentiles[0]]\n",
    "test_Y = test.loc[:, percentiles[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (seq): Sequential(\n",
      "    (0): Linear(in_features=21, out_features=12, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=12, out_features=2, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Simple Mixture Density Network, without handling for missing input features\n",
    "features = train_X.shape[1]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, features, hidden_dim, out_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(features, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        params = self.seq(x)\n",
    "        mu, sigma = torch.tensor_split(params, params.shape[0], dim=0)\n",
    "        \n",
    "        return mu, nnF.elu(sigma)+1+1e-7\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        mu, sigma = self.forward(x)\n",
    "        dist = Normal(mu, sigma)\n",
    "        return -dist.log_prob(y)\n",
    "    \n",
    "mdn = Net(features, 12, 2).double()\n",
    "optimizer = torch.optim.SGD(mdn.parameters(), lr=1e-3)\n",
    "\n",
    "print(mdn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 loss: tensor([0.9963], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "index: 10000 loss: tensor([0.9208], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "index: 20000 loss: tensor([0.9231], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "index: 30000 loss: tensor([0.9211], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "index: 40000 loss: tensor([0.9201], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "index: 50000 loss: tensor([0.9190], dtype=torch.float64, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def train_mdn(mdn, train_X, train_Y, optimizer):\n",
    "    mdn.train()\n",
    "    \n",
    "    train_X = train_X.reset_index().iloc[:, 1:]\n",
    "    train_Y = train_Y.reset_index().iloc[:, 1:].squeeze()\n",
    "\n",
    "    for index, row in train_X.iterrows():\n",
    "        x = torch.tensor(np.double(row.values))\n",
    "        y = torch.tensor(np.double(train_Y.iloc[index]))\n",
    "        \n",
    "        loss = mdn.loss(x, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if index % 10000 == 0:\n",
    "            print(f\"index: {index} loss: {loss}\")\n",
    "\n",
    "train_mdn(mdn, train_X, train_Y, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test loss: 0.9213506576693299\n",
      "Mean squared error: 0.004824049894130483\n"
     ]
    }
   ],
   "source": [
    "def test_mdn(mdn, test_X, test_Y):\n",
    "    mdn.eval()\n",
    "    \n",
    "    test_X = test_X.reset_index().iloc[:, 1:]\n",
    "    test_Y = test_Y.reset_index().iloc[:, 1:].squeeze()\n",
    "\n",
    "    test_loss = 0\n",
    "    sq_er = []\n",
    "    with torch.no_grad():\n",
    "        for index, row in test_X.iterrows():\n",
    "            x = torch.tensor(np.double(row.values))\n",
    "            y = torch.tensor(np.double(test_Y.iloc[index]))\n",
    "            loss = mdn.loss(x, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            mu, sigma = mdn.forward(x)\n",
    "\n",
    "            sq_er.append((mu.item() - y)**2)\n",
    "\n",
    "    test_loss /= test_X.shape[0]\n",
    "    \n",
    "    print(f\"Avg test loss: {test_loss}\")\n",
    "    print(f\"Mean squared error: {np.mean(sq_er)}\")\n",
    "\n",
    "test_mdn(mdn, test_X, test_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mobility-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce1c8699971c3abca39737a4f17ac27a5a89024d987616fa9a36ba7fc3fab9ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
