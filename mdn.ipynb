{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘tract_merged.csv’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Griffin Davis, The University of Texas at Dallas\n",
    "# (C) 2022\n",
    "# Data source:\n",
    "# Chetty, Raj; Friedman, John; Hendren, Nathaniel; Jones, Maggie R.; Porter, Sonya R., 2022, \n",
    "# \"Replication Data for: The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility\", \n",
    "# https://doi.org/10.7910/DVN/NKCQM1, Harvard Dataverse, V1, UNF:6:wwWmCZy1LUqtq02qHdCKFQ== [fileUNF] \n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from importlib import reload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from urllib.parse import urlparse\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "nnF = nn.functional\n",
    "\n",
    "if not os.path.exists('logs'):\n",
    "    os.makedirs('logs')\n",
    "\n",
    "reload(logging) # Notebook workaround\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s [%(threadName)s] [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"logs/mdn.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Download data\n",
    "!wget -nc https://personal.utdallas.edu/~gcd/data/tract_merged.csv\n",
    "ds = pd.read_csv('tract_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subset of columns\n",
    "cols = ['id', 'hhinc_mean2000', 'mean_commutetime2000', 'frac_coll_plus2000', 'frac_coll_plus2010', \n",
    "        'med_hhinc1990', 'med_hhinc2016', 'popdensity2000', 'poor_share2010', 'poor_share2000', \n",
    "        'poor_share1990', 'gsmn_math_g3_2013', 'traveltime15_2010', 'emp2000', 'singleparent_share1990',\n",
    "        'singleparent_share2010', 'singleparent_share2000', \n",
    "        'mail_return_rate2010', 'jobs_total_5mi_2015', 'jobs_highpay_5mi_2015', \n",
    "        'popdensity2010', 'job_density_2013', 'kfr_pooled_pooled_p1', \n",
    "        'kfr_pooled_pooled_p25', 'kfr_pooled_pooled_p50', 'kfr_pooled_pooled_p75', 'kfr_pooled_pooled_p100']\n",
    "\n",
    "excluded = ['rent_twobed2015', 'ln_wage_growth_hs_grad', 'ann_avg_job_growth_2004_2013']\n",
    "\n",
    "full_cols = cols + excluded\n",
    "\n",
    "# Handle null data\n",
    "ds_full = ds[ds.columns[ds.columns.isin(full_cols)]]\n",
    "ds = ds[ds.columns[ds.columns.isin(cols)]]\n",
    "ds = ds.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle split the data into training and test sets (75% / 25%)\n",
    "train, test = train_test_split(ds)\n",
    "\n",
    "train_X = train.loc[:,'hhinc_mean2000':'job_density_2013']\n",
    "test_X = test.loc[:,'hhinc_mean2000':'job_density_2013']\n",
    "\n",
    "percentiles = ['kfr_pooled_pooled_p1', 'kfr_pooled_pooled_p25', 'kfr_pooled_pooled_p50', 'kfr_pooled_pooled_p75']\n",
    "train_Y = train.loc[:, percentiles[1]]\n",
    "test_Y = test.loc[:, percentiles[1]]\n",
    "\n",
    "# Reset indexes and convert Y to pd.Series\n",
    "train_X.reset_index(drop=True, inplace=True)\n",
    "train_Y = train_Y.reset_index(drop=True).squeeze()\n",
    "test_X.reset_index(drop=True, inplace=True)\n",
    "test_Y = test_Y.reset_index(drop=True).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Mixture Density Network, without handling for missing input features\n",
    "features = train_X.shape[1]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, features, hidden_dim, out_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(features, hidden_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        params = self.seq(x)\n",
    "        mu, sigma = torch.tensor_split(params, params.shape[0], dim=0)\n",
    "        \n",
    "        return mu, sigma+1\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        mu, sigma = self.forward(x)\n",
    "        dist = Normal(mu, sigma)\n",
    "        return -dist.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mdn(mdn, X, Y, optimizer, verbose):\n",
    "    mdn.train()\n",
    "    \n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    Y.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for index, row in X.iterrows():\n",
    "        x = torch.tensor(np.double(row.values))\n",
    "        y = torch.tensor(np.double(Y.iloc[index]))\n",
    "        \n",
    "        loss = mdn.loss(x, y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if verbose and index % 10000 == 0:\n",
    "            logging.info(f\"index: {index} loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mdn(mdn, X, Y, verbose):\n",
    "    mdn.eval()\n",
    "\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    Y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    test_loss = 0\n",
    "    sq_er = []\n",
    "    with torch.no_grad():\n",
    "        for index, row in X.iterrows():\n",
    "            x = torch.tensor(np.double(row.values))\n",
    "            y = torch.tensor(np.double(Y.iloc[index]))\n",
    "            loss = mdn.loss(x, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            mu, sigma = mdn.forward(x)\n",
    "\n",
    "            sq_er.append((mu.item() - y)**2)\n",
    "\n",
    "    test_loss /= test_X.shape[0]\n",
    "    \n",
    "    if verbose:\n",
    "        logging.info(f\"Avg test loss: {test_loss}\")\n",
    "        logging.info(f\"Mean squared error: {np.mean(sq_er)}\")\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cab59566b02416d9e41d6366394ee0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension = 3, Loss = 0.2768381263867698\n",
      "Dimension = 4, Loss = 0.2789649501573936\n",
      "Dimension = 5, Loss = 0.29557487171902286\n",
      "Dimension = 6, Loss = 0.29557487171902286\n",
      "Dimension = 7, Loss = 0.27678297429819837\n",
      "Dimension = 8, Loss = 0.27677964461746984\n",
      "Dimension = 9, Loss = 0.2955755801044583\n",
      "Dimension = 10, Loss = 0.2767739615945039\n",
      "Dimension = 11, Loss = 0.2767723332257522\n",
      "Dimension = 12, Loss = 0.27678039662463905\n",
      "Dimension = 13, Loss = 0.27676444063356503\n",
      "Dimension = 14, Loss = 0.29557487171902286\n",
      "Dimension = 15, Loss = 0.2767818423319893\n",
      "Dimension = 16, Loss = 0.27676558481836744\n",
      "Dimension = 17, Loss = 0.2767831932562505\n",
      "Dimension = 18, Loss = 0.2767636130155072\n",
      "Dimension = 19, Loss = 0.2767720795720673\n",
      "Dimension = 20, Loss = 0.29557487171902286\n",
      "Dimension = 21, Loss = 0.27677251045612\n",
      "Dimension = 22, Loss = 0.2767821537030486\n",
      "Dimension = 23, Loss = 0.29557487171902286\n",
      "Dimension = 24, Loss = 0.29557487171902286\n",
      "Dimension = 25, Loss = 0.27674490638982185\n",
      "Dimension = 26, Loss = 0.27677647070538497\n",
      "Dimension = 27, Loss = 0.2767582652135724\n",
      "Dimension = 28, Loss = 0.2767785725211536\n",
      "Dimension = 29, Loss = 0.2767700134633984\n",
      "Selected number of sigmoid activations: 25\n"
     ]
    }
   ],
   "source": [
    "# Determine number of Sigmoid activations to use in hidden layer\n",
    "# 10-fold hyperparameter cross validation using training data\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "losses = {}\n",
    "\n",
    "# Setup folder to save hypervalidation models\n",
    "hyperPath = 'hypervalidation'\n",
    "if not os.path.exists(hyperPath):\n",
    "    os.makedirs(hyperPath)\n",
    "\n",
    "# Try each option for number of Sigmoid activations\n",
    "for hidden_dim in tqdm(range(3, 30)):\n",
    "    # Create network with that hyperparameter\n",
    "    mdn = Net(features, hidden_dim, 2).double()\n",
    "    optimizer = torch.optim.Adam(mdn.parameters(), lr=0.0001)\n",
    "\n",
    "    # Do 10-fold cross validation and store results of tests in array\n",
    "    dim_loss = []\n",
    "    for train_index, test_index in kf.split(train_X):\n",
    "        X_train, X_test = train_X.iloc[train_index], train_X.iloc[test_index]\n",
    "        Y_train, Y_test = train_Y.iloc[train_index], train_Y.iloc[test_index]\n",
    "\n",
    "        train_mdn(mdn, X_train, Y_train, optimizer, False)\n",
    "        dim_loss.append(test_mdn(mdn, X_test, Y_test, False))\n",
    "    \n",
    "    # Store average test results for this hyperparameter option\n",
    "    losses[hidden_dim] = np.mean(dim_loss)\n",
    "\n",
    "    # Save the hypervalidation model\n",
    "    torch.save(mdn, f'{hyperPath}/{hidden_dim}_activ_mdn.pt')\n",
    "\n",
    "    logging.info(\"Dimension = \" + str(hidden_dim) + \", Loss = \" + str(losses[hidden_dim]))\n",
    "\n",
    "# Select best hyperparameter\n",
    "min_loss = np.inf\n",
    "min_dim = 0\n",
    "for hidden_dim in losses:\n",
    "    if losses[hidden_dim] < min_loss:\n",
    "        min_loss = losses[hidden_dim]\n",
    "        min_dim = hidden_dim\n",
    "\n",
    "logging.info(\"Selected number of sigmoid activations: \" + str(min_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 08:44:24,486 [MainThread] [INFO] index: 0 loss: tensor([1.0017], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "2023-03-28 08:44:30,120 [MainThread] [INFO] index: 10000 loss: tensor([1.0762], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "2023-03-28 08:44:35,730 [MainThread] [INFO] index: 20000 loss: tensor([0.9871], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "2023-03-28 08:44:41,284 [MainThread] [INFO] index: 30000 loss: tensor([1.0149], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "2023-03-28 08:44:46,940 [MainThread] [INFO] index: 40000 loss: tensor([0.9803], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "2023-03-28 08:44:52,589 [MainThread] [INFO] index: 50000 loss: tensor([1.0029], dtype=torch.float64, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Retrain with full training dataset (selected 10)\n",
    "mdn = Net(features, 10, 2).double()\n",
    "optimizer = torch.optim.Adam(mdn.parameters(), lr=0.0001)\n",
    "\n",
    "train_mdn(mdn, train_X, train_Y, optimizer, True)\n",
    "\n",
    "# Save the final model\n",
    "torch.save(mdn, f'simple_mdn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 08:44:57,976 [MainThread] [INFO] Avg test loss: 1.0132598407689926\n",
      "2023-03-28 08:44:58,013 [MainThread] [INFO] Mean squared error: 0.18864261512864228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0132598407689926"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with reserved data\n",
    "test_mdn(mdn, test_X, test_Y, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('mobility-models')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce1c8699971c3abca39737a4f17ac27a5a89024d987616fa9a36ba7fc3fab9ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
